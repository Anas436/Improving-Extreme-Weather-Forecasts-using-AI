{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Necessary Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as graph\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file is the preprocessed csv file formed after applying the EDA Techniques of my first notebook. For that refer to the 'Weather.ipynb' notebook. Uploaded on both Google Drive and Slack as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(\"E:/Downloads/updated_train_data.csv\")E:\\Omdena\\Data\n",
    "#dataset = pd.read_csv(\"train_data.csv\")\n",
    "dataset = pd.read_csv(\"E:/Omdena/Data/updated_train_data_for_WeatherII.csv\")\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sample(5)       # Getting a sample view..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The count of the data (especially the first two digits) is exactly similar to that of the number of days in each month. Thus, we have almost same number of data and can use similar weights for training a neural network with respect to months. We do not need to give any specific priority to the months column (its only relevance is that of the time series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The data points are evenly well distributed with time and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['complete'] = [1] * 375734\n",
    "month_file = dataset.groupby(\"Month\")[['complete']]     # Thus was have entire data and no entry is empty...\n",
    "print(month_file.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=\"complete\")\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will check for trends of various columns. Firstly, we will have a look at the mjo1d data and the mei data, since both of them are ratios, so if we could exclude one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = dataset[['startdate', 'mjo1d__phase', 'mjo1d__amplitude']]\n",
    "fig = px.line(phases, x=\"startdate\", y=phases.columns, title=\"MJO Phase and Amplitude Time Series Graph and Regression Line\")\n",
    "fig.update_layout(font_family=\"Courier New\", font_color=\"purple\", title_font_family=\"Times New Roman\", title_font_size=24, title_font_color=\"red\", legend_title_font_color=\"brown\", font_size=20, legend_font_size=12)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since it is clear the Regression lines are intersecting at some point if we traverse the graph backwards. So to get a more clear Relation, we will scale both of the features to MinMax [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler      # Scaler used for scaling...\n",
    "'''There are many types of Scalers available like MinMax, Standard, etc.'''\n",
    "MinMax = MinMaxScaler()\n",
    "phases['mjo1d__phase'] = MinMax.fit_transform(np.array(phases['mjo1d__phase']).reshape(-1, 1))\n",
    "phases['mjo1d__amplitude'] = MinMax.fit_transform(np.array(phases['mjo1d__amplitude']).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(phases, x=\"startdate\", y=phases.columns, title=\"MJO Phase and Amplitude Time Series Graph and Regression Line MinMax Scaled\")     # Creating a line plot...\n",
    "fig.update_layout(font_family=\"Courier New\", font_color=\"purple\", title_font_family=\"Times New Roman\", title_font_size=24, title_font_color=\"red\", legend_title_font_color=\"brown\", font_size=20, legend_font_size=12)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we similarly check for the MEI Attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ranges of values are too far apart we cannot make a clear conclusion of the variables so we will scale them down to a common range. The Best scaler for this purpose in MinMaxScaler, since none of the below variables have negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases1 = dataset[['startdate', 'mei__mei', 'mei__meirank', 'mei__nip']]\n",
    "fig = px.line(phases1, x=\"startdate\", y=phases1.columns, title=\"MEI Attributes Time Series Graph and Regression Line\")\n",
    "fig.update_layout(font_family=\"Courier New\", font_color=\"purple\", title_font_family=\"Times New Roman\", title_font_size=24, title_font_color=\"red\", legend_title_font_color=\"brown\", font_size=20, legend_font_size=12)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases1['mei__mei'] = MinMax.fit_transform(np.array(phases1['mei__mei']).reshape(-1, 1))\n",
    "phases1['mei__meirank'] = MinMax.fit_transform(np.array(phases1['mei__meirank']).reshape(-1, 1))\n",
    "phases1['mei__nip'] = MinMax.fit_transform(np.array(phases1['mei__nip']).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(phases1, x=\"startdate\", y=phases1.columns, title=\"MEI Attributes Time Series Graph and Regression Line MinMax Scaled\")\n",
    "fig.update_layout(font_family=\"Courier New\", font_color=\"purple\", title_font_family=\"Times New Roman\", title_font_size=24, title_font_color=\"red\", legend_title_font_color=\"brown\", font_size=20, legend_font_size=12)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above graph it is clear that the mei_nip has only binary values (either 0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now after performing the MinMax Scaling we see that the MJO Attributes are not correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrelationMatrix():\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(\"Correlation of MJO and MEI Ratio\", color=\"red\", size=16)\n",
    "    sns.heatmap(phases.corr(), cmap=\"magma\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "phases['Temperature'] = dataset['contest-tmp2m-14d__tmp2m']\n",
    "phases['Mei_Mei'] = phases1['mei__mei']\n",
    "phases['Mei_Nip'] = phases1['mei__nip']\n",
    "phases['Mei_Rank'] = phases1['mei__meirank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorrelationMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuitions from the HeatMap (After MinMax Scaling of Values) :-\n",
    "1. The MJO Attributes are almost uncorrelated to each other, but they have same significance (correlation) to that of Temperature (-0.13 and -0.12). So we can eradicate one of them since they amount to same weights in evaluating Temperature.\n",
    "2. Also, all the MEI Attributes are well correlated to one another (0.93, 0.81 and 1). In terms of relation to temperature, we have (0.062, 0.2, 0.082) respectively. Thus we only keep 0.2 as the column suitable for computation.\n",
    "3. Since these values are MinMax scaled, applying weights for Deep Learning (SGD - Stochastic Gradient Descent) will work more effectively and quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroppingColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     # Dropping the Irrelevant columns...\n",
    "        X = X.drop(columns=\"mei__meirank\", axis=1)\n",
    "        X = X.drop(columns=\"mei__mei\", axis=1)\n",
    "        X = X.drop(columns=\"mjo1d__amplitude\", axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaling(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):     # Scaling down the features...\n",
    "        Scaler = MinMaxScaler()\n",
    "        X['mjo1d__phase'] = Scaler.fit_transform(np.array(X['mjo1d__phase']).reshape(-1, 1))\n",
    "        X['mei__nip'] = Scaler.fit_transform(np.array(X['mei__nip']).reshape(-1, 1))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X     # Since we are not transforming the dataset, the transform function is kept empty..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipe = Pipeline([      # Using Pipeline to call the specific classes...\n",
    "    (\"Remove\", DroppingColumns()),\n",
    "    (\"MinMax\", MinMaxScaling())\n",
    "])\n",
    "dataset = Pipe.fit_transform(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of Features are reduced from 45 to 42."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Seasons in USA are clustered in group of three months each. Though they do not comprise the entirety of each month, these are the meteorological seasons:\n",
    "1. Spring months: March, April, May\n",
    "2. Summer months: June, July, August\n",
    "3. Fall months: September, October, November\n",
    "4. Winter months: December, January, February"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will do an Analysis for Spring Season (March, April and May).\n",
    "    Since the || and && are ambiguous for dataframe, we use bitwise | and & which are overloaded operators for pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spring = dataset[(dataset['Month'] == 3) | (dataset['Month'] == 4) | (dataset['Month'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenamingWinds(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['Height10'] = X['contest-wind-h10-14d__wind-hgt-10']\n",
    "        X['Height100'] = X['contest-wind-h100-14d__wind-hgt-100']\n",
    "        X['Height500'] = X['contest-wind-h500-14d__wind-hgt-500']\n",
    "        X['Height850'] = X['contest-wind-h850-14d__wind-hgt-850']\n",
    "        X['LongitudinalWind925'] = X['contest-wind-vwnd-925-14d__wind-vwnd-925']\n",
    "        X['LongitudinalWind250'] = X['contest-wind-vwnd-250-14d__wind-vwnd-250']\n",
    "        X['ZonalWind250'] = X['contest-wind-uwnd-250-14d__wind-uwnd-250']\n",
    "        X['ZonalWind925'] = X['contest-wind-uwnd-925-14d__wind-uwnd-925']\n",
    "        X = X.drop(['contest-wind-h10-14d__wind-hgt-10', 'contest-wind-h100-14d__wind-hgt-100', 'contest-wind-h500-14d__wind-hgt-500', 'contest-wind-h850-14d__wind-hgt-850', 'contest-wind-vwnd-925-14d__wind-vwnd-925', 'contest-wind-vwnd-250-14d__wind-vwnd-250', 'contest-wind-uwnd-250-14d__wind-uwnd-250', 'contest-wind-uwnd-925-14d__wind-uwnd-925'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenamingTemperatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['Mean0Temperature34w'] = X['nmme0-tmp2m-34w__nmme0mean']    # Most Recent Mean...\n",
    "        X['Temperature'] = X['contest-tmp2m-14d__tmp2m']\n",
    "        X['MeanTemperature34w'] = X['nmme-tmp2m-34w__nmmemean']\n",
    "        X['Nmme0Mean'] = X['nmme0mean']        # NMME Mean...\n",
    "        X['Evaporation'] = X['contest-pevpr-sfc-gauss-14d__pevpr']\n",
    "        X['RecentCancmPrate56w'] = X['nmme0-prate-56w__cancm40']    # Cancm40 is taken...\n",
    "        X['RecentCcsmPrate56w'] = X['nmme0-prate-56w__ccsm30']     # Ccsm30 is taken...\n",
    "        X['CancmPrate56w'] = X['nmme-prate-56w__cancm4']              # Cancm4...\n",
    "        X['Ccsm3Prate56w'] = X['nmme-prate-56w__ccsm3']                # Ccsm3...\n",
    "        X = X.drop(['nmme0-tmp2m-34w__nmme0mean', 'contest-tmp2m-14d__tmp2m', 'nmme0mean', 'nmme-tmp2m-34w__nmmemean', 'contest-pevpr-sfc-gauss-14d__pevpr', 'nmme0-prate-56w__cancm40', 'nmme0-prate-56w__ccsm30', 'nmme-prate-56w__cancm4', 'nmme-prate-56w__ccsm3'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenamingPrecipitation(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['Humidity'] = X['contest-rhum-sig995-14d__rhum']\n",
    "        X['SeaLevelPressure'] = X['contest-slp-14d__slp']\n",
    "        X['Pressure'] = X['contest-pres-sfc-gauss-14d__pres']\n",
    "        X['AtmosphericPrecipitate'] = X['contest-prwtr-eatm-14d__prwtr']\n",
    "        X['Precipitation'] = X['contest-precip-14d__precip']\n",
    "        X['Elevation'] = X['elevation__elevation']\n",
    "        X['Regions'] = X['climateregions__climateregion']\n",
    "        X = X.drop(['contest-rhum-sig995-14d__rhum', 'contest-slp-14d__slp', 'contest-pres-sfc-gauss-14d__pres', 'contest-precip-14d__precip', 'elevation__elevation', 'climateregions__climateregion', 'contest-prwtr-eatm-14d__prwtr'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElNinoFactors(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['ElNinoLongitudinalWind250Mean'] = X['wind-vwnd-250-2010-mean']\n",
    "        X['ElNinoZonalWind250Mean'] = X['wind-uwnd-250-2010-mean']\n",
    "        X['ElNinoZonalWind925Mean'] = X['wind-uwnd-925-2010-mean']\n",
    "        X['ElNinoLongitudinalWind925Mean'] = X['wind-vwnd-925-2010-mean']\n",
    "        X['GlacierFactorMean'] = X['icec-2010-mean']\n",
    "        X['ElNinoHeight10Mean'] = X['wind-hgt-10-2010-mean']\n",
    "        X['ElNinoHeight100Mean'] = X['wind-hgt-100-2010-mean']\n",
    "        X['ElNinoHeight500Mean'] = X['wind-hgt-500-2010-mean']\n",
    "        X['ElNinoHeight850Mean'] = X['wind-hgt-850-2010-mean']\n",
    "        X['ElNinoSeaTemperatureMean'] = X['sst-2010-mean']\n",
    "        X = X.drop(['wind-vwnd-250-2010-mean', 'wind-uwnd-925-2010-mean', 'wind-uwnd-250-2010-mean', 'wind-vwnd-925-2010-mean', 'icec-2010-mean', 'wind-hgt-850-2010-mean', 'wind-hgt-500-2010-mean', 'wind-hgt-100-2010-mean', 'wind-hgt-10-2010-mean', 'sst-2010-mean'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This Pipeline is just used for naming of columns to make the understanding of the variables a bit more clear. Otherwise, it was quite difficult to understand the general nomenclature of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ColumnNameTransformationPipe = Pipeline([\n",
    "    (\"RenameI\", RenamingWinds()),\n",
    "    (\"RenameII\", RenamingTemperatures()),\n",
    "    (\"RenameIII\", RenamingPrecipitation()),\n",
    "    (\"RenameIV\", ElNinoFactors())\n",
    "])\n",
    "SpringSeason = ColumnNameTransformationPipe.fit_transform(Spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpringSeason.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will check the Environmental Factors Correlation To Temperature in Spring Season via HeatMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = SpringSeason[['Temperature', 'Height10', 'Height100', 'Height500', 'Height850', 'LongitudinalWind925', 'LongitudinalWind250', 'ZonalWind250', 'ZonalWind925', 'Mean0Temperature34w' , 'MeanTemperature34w', 'Nmme0Mean', 'Elevation']]\n",
    "\n",
    "def Plotting(variable):\n",
    "    plt.figure(figsize=(10, 7.5))\n",
    "    plt.title(\"Correlation in Spring Season\", color=\"red\", size=24)\n",
    "    sns.heatmap(variable.corr(), cmap=\"icefire\", annot=True)\n",
    "    plt.show()\n",
    "\n",
    "Plotting(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuitions from the Correlation Matrix-\n",
    "1. Geo-potential Height depends upon the Temperature and Pressure of the region but it does not depend upon the Elevation.\n",
    "2. Mean0Temperature34w (recent) is highly correlated to MeanTemperature34w (0.97) and similarly for Nmme0Mean (0.93). Thus we will MinMax scale them and check their correlation again.\n",
    "3. Height100 and Height500 are well correlated (0.94) so we can exclude one of them.\n",
    "4. Greater the Elevation, lower will be the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = SpringSeason[['Elevation', 'Regions', 'startdate', 'Temperature']]\n",
    "fig = px.histogram(plot, x=\"Regions\", title=\"Data Points Count in Spring Season\")           # The Cold Semi-Arid Climate is the most common one...\n",
    "fig.update_layout(title_font_color=\"red\", title_font_size=24, font_color=\"purple\", font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition-\n",
    "1. Since the Cold Semi-Arid Region data prevails in the Spring Season, there must be a wide thermal variation between day and night, lower amount of precipitation than evaporation and shortage of humidity (the wind is mostly dry). These factors are the most prevailing factors in the Cold Semi-Arid Regions.\n",
    "2. The Semi-Arid data points amount to more than 50% of total data points in the Spring Season. Thus, while training our Deep learning or Machine Learning Model the SGD (Stochastic Gradient Descent) can probably have a faster learning rate influenced by the BSk data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(SpringSeason, x=\"startdate\", y=\"Temperature\", histnorm='probability density', title=\"Probability Density of Spring Season of Two Years (Showing a generalized Increasing Trend)\")\n",
    "fig.update_layout(bargap=0.1, title_font_size=24, title_font_color=\"red\", font_size=16, font_color=\"purple\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition-\n",
    "1. Since the Probability of both years shows a generalized increase in Temperature Probability. It very well follows the generalized Normal Distribution curve.\n",
    "2. Thus we can Normally scale (Standard Scaler) to scale down the temperature variables and the variables closely affected by the Temperature (Normal Distribution:- Mean:0, Variance:1). Thus, we can scale down all the Temperature variables by Standard Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = SpringSeason[['Temperature', 'Evaporation' ,'RecentCancmPrate56w', 'RecentCcsmPrate56w', 'CancmPrate56w', 'Ccsm3Prate56w', 'Humidity', 'SeaLevelPressure', 'Pressure', 'AtmosphericPrecipitate', 'Precipitation', 'Elevation']]\n",
    "Plotting(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition -\n",
    "1. The Pressure and Elevation are highly negatively correlated ( -0.92 ) and they have same correlation or rather the almost the same affect on Temperature ( -0.47 and -0.41 ), so we can eradicate any one of them.\n",
    "2. Since other factors are not well correlated to any of them, we should not remove any one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check for ElNino Factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var2 = SpringSeason[['Temperature', 'ElNinoLongitudinalWind250Mean', 'ElNinoZonalWind250Mean', 'ElNinoZonalWind925Mean', 'ElNinoLongitudinalWind925Mean', 'GlacierFactorMean', 'ElNinoHeight10Mean', 'ElNinoHeight100Mean', 'ElNinoHeight500Mean', 'ElNinoHeight850Mean', 'ElNinoSeaTemperatureMean']]\n",
    "Plotting(var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition -\n",
    "1. The ElNinoHeight columns are very closely related to each other thus, we will only take one of them and remove the rest, we will keep ElNinoHeight10Mean and ElNinoHeight850Mean.\n",
    "2. The Longitudinal and Zonal Wind 250 have the almost same correlation values with Temperature (0.38 and 0.39) such that they have the same degree of influence on the Temperature, thus we will remove the which is more correlated to the other variables (ElNinoZonalWind250Mean)\n",
    "3. Also the ZonalWinds are more correlated to Goe-potential Height than the LongitudinalWinds, which means, the Wind flowing in Horizontal Direction poses more impact on other features than the Vertical wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemovingColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.drop(['MeanTemperature34w', 'Nmme0Mean', 'Height500', 'Pressure', 'ElNinoHeight100Mean', 'ElNinoHeight500Mean', 'ElNinoZonalWind250Mean'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilteringPipe = Pipeline([\n",
    "    (\"Removing\", RemovingColumns())\n",
    "])\n",
    "SpringSeason = FilteringPipe.fit_transform(SpringSeason)\n",
    "SpringSeason.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of Columns have been reduced from 42 to 35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now we will remove the Time series data to have a more general correlation view of various parameters for the Spring Season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDataExclusion(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     # We removed the time series data...\n",
    "        X = X.drop(columns=\"startdate\", axis=1)\n",
    "        X = X.drop(columns=\"date\", axis=1)\n",
    "        X = X.drop(['Month', 'Year', 'lat', 'lon'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Latitudes and Longitudes are constant with and not variable with Time Series data, they can be simply passed along with the weights, to ascertain the coordinates. Just a simple reminder to Geography, the latitudes and longitudes do not influence any environmental factors, they are just to represent the coordinates on a Map or a Globe. So they can be included with loss function or the metrics while training the Deep Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExclusionPipe = Pipeline([\n",
    "    (\"Exclude\", TimeDataExclusion())\n",
    "])\n",
    "SpringSeason1 = ExclusionPipe.transform(SpringSeason)\n",
    "SpringSeason1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now the controlling factors have been reduced from 35 to 29."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the correlation matrix of the entire factors as whole to see their influence on Temperature during the Spring Season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 13))\n",
    "plt.title(\"Correlation Of Various Factors In Spring Season\", color=\"blue\", size=24)\n",
    "sns.heatmap(SpringSeason1.corr(), cmap=\"Spectral\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the data is time series data, the correlation values are already in the same time frame such that the values are calculated by taking the time into consideration since, the data is already sorted in a continous flow according to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition -\n",
    "1. SeaLevelPressure and GlacierFactorMean have same correlation with respect to Temperature (-0.47 and -0.46).\n",
    "2. Height850 and ElNinoSeaTemperatureMean have pretty similar correlation with respect to Temperature (0.42 and 0.43). This means that the Geo-potential Height at 850 millibar and ElNinoSeaTemperature has the same effect to the Temperature of the regions, thus the Regions of Height850 are pretty well influenced by the ElNinoSeaTemperatureMean.\n",
    "3. ElNinoHeight10Mean is very negatively correlated to Height10 (-0.89) and they have same magnitude of effect on Temperature (0.52 and -0.52). Thus, we can exclude one of them since they have same degree of effect and are negatively correlated to each other.\n",
    "4. MJO Ratio is not well related to any factor and neither the Temperature as well but the MEI Ratio has an enhanced relation to other factors and the Temperature as compared to that of MJO Ratio. Thus, from the heatmap it would be preferred that we take MEI Ratio and exclude MJO Ratio.\n",
    "5. Height850 and AtmosphericPrecipitate have same correlation with Height100 (0.61). Height100 is fairly well correlated to Temperature (0.79). So, since the two factors have same correlation values (0.61) to a variable we can exclude one of them from the Computation.\n",
    "6. LongitudinalWind925 and LongitudinalWind250 have same correlation with the Mean0Temperature34w (0.34), so we can exclude one of them and it also gives an idea, that the LongitudinalWinds provided have the similar properties (not exactly same).\n",
    "7. Humidity, RecentCcsmPrate56w and SeaLevelPressure have same effect on the Height100 (-0.32). This means, that for Spring Season our chosen weather station gives the correct 56 week precipitate and has the same influence on the Height100 as that of the Humidity and SeaLevelPressure (the two main factors on which the Precipitation depends).\n",
    "8. The ZonalWind925 and Elevation have the same correlation values with respect to Mean0Temperature34w (-0.35). This gives us a glimpse of idea that at the Zonal Winds and the Elevation has the same effect on the Computation of Temperature by the weather forecasts.\n",
    "9. LongitudinalWind925 and ElNinoZonalWind925 have exactly same correlation values on the Temperature (0.38). Thus, there is a high chance that these two winds have the same characteristics and play the same role in determining the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The columns which we will further remove:-\n",
    "    GlacierFactorMean, ElNinoSeaTemperatureMean, ElNinoHeight10Mean, mjo1d__phase, AtmosphericPrecipitate, LongitudinalWind925, SeaLevelPressure, Humidity, ZonalWind925, ElNinoZonalWind925, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemovingColumns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):      # Pipeline created to remove the further irrelevant columns...\n",
    "        X = X.drop(columns=\"GlacierFactorMean\", axis=1)\n",
    "        X = X.drop(columns=\"ElNinoSeaTemperatureMean\", axis=1)\n",
    "        X = X.drop(columns=\"ElNinoHeight10Mean\", axis=1)\n",
    "        X = X.drop(columns=\"mjo1d__phase\", axis=1)\n",
    "        X = X.drop(columns=\"AtmosphericPrecipitate\", axis=1)\n",
    "        X = X.drop(columns=\"LongitudinalWind925\", axis=1)\n",
    "        X = X.drop(columns=\"SeaLevelPressure\", axis=1)\n",
    "        X = X.drop(columns=\"Humidity\", axis=1)\n",
    "        X = X.drop(columns=\"ElNinoZonalWind925Mean\", axis=1)\n",
    "        X = X.drop(columns=\"ZonalWind925\", axis=1)\n",
    "        X = X.drop(columns=\"RecentCancmPrate56w\", axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ColumnRemovePipe = Pipeline([\n",
    "    (\"Removing\", RemovingColumns())\n",
    "])\n",
    "SpringSeason1 = ColumnRemovePipe.transform(SpringSeason1)\n",
    "SpringSeason1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of Significant columns have been reduced from 29 to 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The below graph shows the general temperature trend. Take a look of a year and we will find a bell shaped curve similar to that of the Normal Distribution curve. The little noise in the left domain can easily be rectified by including a squared bias function in the Deep Learning Model while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(dataset, x=\"startdate\", y=\"contest-tmp2m-14d__tmp2m\", histnorm='probability density', title=\"Probability Temperature Density of All Season of Two Years (A Generalized Normal Distribution)\")\n",
    "fig.update_layout(bargap=0.1, title_font_size=24, title_font_color=\"red\", font_size=16, font_color=\"purple\", height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(dataset, x=\"startdate\", y=\"nmme0-tmp2m-34w__nmme0mean\", histnorm='probability density', title=\"Probability Temperature Density of Chosen Station All Season of Two Years (A Generalized Normal Distribution)\")\n",
    "fig.update_layout(bargap=0.1, title_font_size=24, title_font_color=\"red\", font_size=16, font_color=\"purple\", height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Looking at the above two graphs this gives us an intuition that if we scale them down using Standard Scaler, both of them will become more continuous and correlated with each other. Even before scaling them show a very clear positive correlation of 0.92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpringSeason1['latitude'] = SpringSeason['lat']     # Latitudes added...\n",
    "SpringSeason1['longitude'] = SpringSeason['lon']    # Longitudes added...\n",
    "plt.figure(figsize=(15, 13))\n",
    "plt.title(\"Final HeatMap of Factors Of Spring Season\", color=\"red\", size=22)\n",
    "sns.heatmap(SpringSeason1.corr(), cmap=\"magma\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuitions:-\n",
    "1. Latitude is significantly negatively correlated to the Temperature and Evaporation Parameters.\n",
    "2. Longitude is negatively correlated to Recent Data of Ccsm station.\n",
    "3. Important:- Almost every factor has a 20% effect on Temperature calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpringSeason1['Startdate'] = SpringSeason['startdate']    # Start date and regions also added...\n",
    "SpringSeason1['regions'] = SpringSeason['Regions']\n",
    "SpringSeason1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_contour(SpringSeason1, x=\"Mean0Temperature34w\", y=\"Height100\", title=\"Temperature0 X Height100\")\n",
    "fig.update_traces(contours_coloring=\"fill\", contours_showlabels = True, colorscale=\"viridis\")\n",
    "fig.update_layout(width=1000, height=650, font_family=\"Courier New\", font_color=\"green\",     title_font_family=\"Times New Roman\", title_font_color=\"red\", title_font_size=24, font_size=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one Intensity Point (Maxima) and is a global one, so we can use either exponential scaling or the Logarithmic Scaling (its inverse) with respect to Height100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_contour(SpringSeason1, x=\"Mean0Temperature34w\", y=\"latitude\", title=\"Temperature0 X Latitude\")\n",
    "fig.update_traces(contours_coloring=\"fill\", contours_showlabels = True, colorscale=\"Thermal\")\n",
    "fig.update_layout(width=1000, height=650, font_family=\"Courier New\", font_color=\"green\",     title_font_family=\"Times New Roman\", title_font_color=\"red\", title_font_size=24, font_size=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two local Maxima and thus we can provide more bias and weight regarding the latitudes Maxima. The Maxima is pretty dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_contour(SpringSeason1, x=\"Mean0Temperature34w\", y=\"longitude\", title=\"Temperature0 X Longitude\")\n",
    "fig.update_traces(contours_coloring=\"fill\", contours_showlabels = True, colorscale=\"Thermal\")\n",
    "fig.update_layout(width=1000, height=650, font_family=\"Courier New\", font_color=\"green\",     title_font_family=\"Times New Roman\", title_font_color=\"red\", title_font_size=24, font_size=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we have two local Maxima for Longitudes and then weigh them accordingly as per the Longitude. Also, here in this case, we have heavy Maxima (heavy Intensity point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Intensity of Maxima determines the learning rate of the Deep Learning Algorithm especially for the SGD and Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "87970ed34195fef68c8464c6d4e1428c2376dd29de258c9c3dc6f07b1b1cbaab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
