{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vKHSQTQp4QJ"
   },
   "source": [
    "#### Importing Libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9V0e9Eydp4QV"
   },
   "outputs": [],
   "source": [
    "import numpy as np                                       # Numpy for arrays...\n",
    "import pandas as pd                                      # Pandas for the datasets...\n",
    "import seaborn as sns                                    # Seaborn for visualization...\n",
    "import matplotlib.pyplot as plt                          # Matplotlib for visualization...\n",
    "from sklearn.base import BaseEstimator, TransformerMixin      # Transformer classes for Pipelining... \n",
    "from sklearn.impute import SimpleImputer                    # SimpleImputer to compute the missing values...\n",
    "import missingno as missing                               # Visualization of Missing values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Wjr1JKvp4Qa"
   },
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(\"E:/Downloads/train_data.csv\")\n",
    "dataset = pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnGq9nSYp4Qb",
    "outputId": "b36fec75-a3aa-421a-e70c-0c65e181c113"
   },
   "outputs": [],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idM-XltNp4Qe"
   },
   "source": [
    "#### Geographical Concepts:- \n",
    "1. NMME - North America Multi-Model Ensemble.\n",
    "2. Temperature is affected by wind. Wind is affected by Atmospheric Pressure and Topography (relative height from sea level). contest-wind-h10-14d__wind-hgt-10 means that the temperature values are provided with varying pressure and height respectively. Wind can make the area warmer or cooler and it depends upon the region from where the arriving from.\n",
    "3. contest-pevpr-sfc-gauss-14d__pevpr denotes the Evaporation. Temperature is directly proportional to the rate of Evaporation.\n",
    "4. contest-rhum-sig995-14d__rhum denotes the relative humidity of the data. The Relative Humidity is inversely proportional to the Temperature.\n",
    "5. The values as 56w and 34w are weeks like 5-6 weeks and 3-4 weeks.\n",
    "6. The total Precipitable water is the precipitation that can occur on a region. Precipitation is the precipitation occured in the region.\n",
    "7. MEI in Geography stands for Maps Etc. Inc.\n",
    "8. The Sea Surface Temperature (SST) defines the Sea Temperature at the surface and is a prime factor for Temperature determination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up1u6zFip4Qh"
   },
   "source": [
    "#### Below Explanation is my own explanation as perceived from the dataset and via internet. Feel free to provide any correction or updation. Now We will group the columns. I am providing the description of the column names for everybody reference:-\n",
    "1. index for the index.\n",
    "2. lat for the lattitudes, lon for the longitudes.\n",
    "3. we have a start date column.\n",
    "contest-pevpr-sfc-gauss-14d__pevpr column for evaporation.\n",
    "4. Digged into net and NMME stands for North America Multi-Model Ensemble, and 10 row enteries after the contest-pevpr-sfc-gauss-14d__pevpr column deal with the evaporation data. The last column of Evaporation data deals with the mean values of the Weather Forecast Stations.\n",
    "5. contest-wind-h10-14d__wind-hgt-10 column deals with the Wind. Similarly the 10 row enteries after the contest-wind-h10-14d__wind-hgt-10 column deal with the Height data and we have the last Height column as the mean data.\n",
    "6. contest-rhum-sig995-14d__rhum is the relative humidity data. The next 10 row enteries deal with the Relative Humidity and the last column with the Mean.\n",
    "7. contest-wind-h100-14d__wind-hgt-100 is another column dealing with the wind. We have total 20 columns for this particular data. The first 10 columns deals with p-rate 54w and the next 10 columns deal with p-rate 34w.\n",
    "8. contest-tmp2m-14d__tmp2m, contest-slp-14d__slp are probably the ratio of the atmospheric factors.\n",
    "contest-wind-vwnd-925-14d__wind-vwnd-925 is the data for longitudinal wind. It also has 10 rows of data succedding it with last row as mean data. contest-wind-uwnd-250-14d__wind-uwnd-250 Also deals with the wind (longitudinal) with different atmospheric factors and time duration.\n",
    "9. contest-prwtr-eatm-14d__prwtr is the precipitable water for entire atmosphere.\n",
    "10. contest-precip-14d__precip deals with the precipitation.\n",
    "11. climateregions__climateregion, elevation__elevation gives the idea of the climate region and the precipitation.\n",
    "12. mjo1d__phase, mjo1d__amplitude are the MJO phase and amplitudes.\n",
    "13. mei__mei, mei__meirank, mei__nip is the MEI system.\n",
    "14. sst-2010-1, sst-2010-2, sst-2010-3, sst-2010-4, sst-2010-5, sst-2010-6, sst-2010-7, sst-2010-8, sst-2010-9, sst-2010-10 are the 10 columns of the Sea-Surface-Temperature.\n",
    "15. The other columns are the different values of wind with different atmospheric pressures and are in a group of 10 or 20 columns each. Thus we can create a seperate column for the mean of each Wind values of unique conditions.\n",
    "16. wind-hgt columns have two groups of 10 columns each. One group corresponds to 500m and 850m above sea level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J_thABrp4Qj",
    "outputId": "e01f6c4b-3f79-4020-b32e-3305e9245cb7"
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for column in dataset.columns:\n",
    "    if dataset[column].isnull().any():\n",
    "        lst.append(column)\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcFOsQMFp4Qm"
   },
   "source": [
    "#### Later, we will find that these all values are dropped, due to being insignificant in computing the temperature, so I simply used the mean strategy to compute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnvibqkXp4Qo"
   },
   "source": [
    "#### Let us check whether the missing values are a case of NMAR (Not Missing at Random), MCAR (Missing Completely at Random) OR MAR (Missing at Random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "197qVnG0p4Qr",
    "outputId": "7f87ac59-79e7-47da-fe86-da5d44013035"
   },
   "outputs": [],
   "source": [
    "missing.matrix(dataset)       # Visualizing the missing values..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbFz6HmZp4Qs"
   },
   "source": [
    "#### Looking at the graph it is clear, that it is a case of NMAR, like we have them missing values all lying in a small range of time say a month probably. Also, they are separated into two groups in time series (according to the time). Thus, according to my intuition, the missing values are separated by a Year and both the groups of missing values lie in the same month, thus it is case of NMAR (Not Missing at Random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibeK2S9jp4Qu"
   },
   "source": [
    "#### First Let us break startdate column to month and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnPhlH4pp4Qu"
   },
   "outputs": [],
   "source": [
    "class DateToMonthAndYears(BaseEstimator, TransformerMixin):     # Using Pipelining for faster throughput...\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X['startdate'] = pd.to_datetime(X['startdate'])         # Converting to date and time...\n",
    "        X['date'] = pd.DatetimeIndex(X['startdate']).date\n",
    "        X['Month'] = pd.DatetimeIndex(X['startdate']).month\n",
    "        X['Year'] = pd.DatetimeIndex(X['startdate']).year\n",
    "        return X.drop(columns=\"startdate\")     # Dropping the startdate column..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbJMsdIIp4Qv"
   },
   "source": [
    "#### Now Let us Remove the Index Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxeJUAnnp4Qw"
   },
   "outputs": [],
   "source": [
    "class RemoveIndex(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=\"index\")    # We drop the Index Column..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1p6qNC4Ip4Qx"
   },
   "source": [
    "#### Now, we will find all the columns with the null values and then compute the null values using the Mean strategy of the Simple Imputer from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRTXUeTtp4Qy"
   },
   "outputs": [],
   "source": [
    "null_cols = []\n",
    "class ObtainingNullValues(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        missing_values = []\n",
    "        for col in dataset.columns:\n",
    "            if(dataset[col].isnull().any()):    # All columns have missing values as float values...\n",
    "                null_cols.append(col)\n",
    "                missing_values.append(dataset[col].isnull().sum())   # The maximum missing value is 15,934 values in a single column out of 3,75,734 rows...\n",
    "        # Thus if we evaluate the percentage of maximum missing values in a column, it accounts to 4.24 % only... Thus we can take the missing values as the mean of the given values in the column...\n",
    "        Imputer = SimpleImputer(strategy=\"mean\")\n",
    "        for col in null_cols:\n",
    "            X[col] = Imputer.fit_transform(X[[col]])     # Now we set the null values as the mean of the dataset column...\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3tIB7vWp4Qz"
   },
   "source": [
    "#### Evaluating the means of each row for every year for El Nino phenomenon for the Year 2010-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33zJdhpTp4Q0"
   },
   "outputs": [],
   "source": [
    "class TwentyColumnSet(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        wind, wind1, wind2, wind3 = [], [], [], []\n",
    "        X['wind-vwnd-250-2010-mean'] = 0     # Creating an empty column for MEAN...\n",
    "        X['wind-uwnd-250-2010-mean'] = 0     \n",
    "        X['wind-uwnd-925-2010-mean'] = 0      \n",
    "        X['wind-vwnd-925-2010-mean'] = 0\n",
    "        for i in range(1, 21):\n",
    "            wind.append('wind-vwnd-250-2010-{a}'.format(a=i))    # Using format string to store the column names...\n",
    "            wind1.append('wind-uwnd-250-2010-{a}'.format(a=i))\n",
    "            wind2.append('wind-uwnd-925-2010-{a}'.format(a=i))\n",
    "            wind3.append('wind-vwnd-925-2010-{a}'.format(a=i))    # Using format string...\n",
    "        for j in range(0, len(wind)):\n",
    "            X['wind-vwnd-250-2010-mean'] = X['wind-vwnd-250-2010-mean'] + X[wind[j]]\n",
    "            X['wind-uwnd-250-2010-mean'] = X['wind-uwnd-250-2010-mean'] + X[wind1[j]]\n",
    "            X['wind-uwnd-925-2010-mean'] = X['wind-uwnd-925-2010-mean'] + X[wind2[j]]\n",
    "            X['wind-vwnd-925-2010-mean'] = X['wind-vwnd-925-2010-mean'] + X[wind3[j]]\n",
    "        X['wind-vwnd-250-2010-mean'] = X['wind-vwnd-250-2010-mean'] / 20    # Storing the mean...\n",
    "        X['wind-uwnd-250-2010-mean'] = X['wind-uwnd-250-2010-mean'] / 20\n",
    "        X['wind-uwnd-925-2010-mean'] = X['wind-uwnd-925-2010-mean'] / 20\n",
    "        X['wind-vwnd-925-2010-mean'] = X['wind-vwnd-925-2010-mean'] / 20\n",
    "        for j in range(0, len(wind)):\n",
    "            X = X.drop(columns=\"wind-vwnd-250-2010-{a}\".format(a=j+1))    # Dropping the irrelevant columns...\n",
    "            X = X.drop(columns=\"wind-uwnd-250-2010-{a}\".format(a=j+1))\n",
    "            X = X.drop(columns=\"wind-uwnd-925-2010-{a}\".format(a=j+1))\n",
    "            X = X.drop(columns=\"wind-vwnd-925-2010-{a}\".format(a=j+1))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRsT7KUDp4Q1"
   },
   "source": [
    "#### Finding the Yearly Mean of every day for the El Nino parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIJ3zz-Op4Q2"
   },
   "outputs": [],
   "source": [
    "class TenColumnSet(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        set1, set2, set3, set4, set5, set6 = [], [], [], [], [], []\n",
    "        X['icec-2010-mean'] = 0\n",
    "        X['wind-hgt-850-2010-mean'] = 0    # Creating columns for storing the mean of the values...\n",
    "        X['wind-hgt-10-2010-mean'] = 0\n",
    "        X['wind-hgt-100-2010-mean'] = 0\n",
    "        X['wind-hgt-500-2010-mean'] = 0\n",
    "        X['sst-2010-mean'] = 0\n",
    "        for i in range(1,11):\n",
    "            set1.append('icec-2010-{a}'.format(a=i))          # Using format string to store the column name...\n",
    "            set2.append('wind-hgt-850-2010-{a}'.format(a=i))\n",
    "            set3.append('wind-hgt-10-2010-{a}'.format(a=i))\n",
    "            set4.append('wind-hgt-100-2010-{a}'.format(a=i))\n",
    "            set5.append('wind-hgt-500-2010-{a}'.format(a=i))\n",
    "            set6.append('sst-2010-{a}'.format(a=i))\n",
    "        for j in range(0, len(set1)):\n",
    "            X['icec-2010-mean'] = X['icec-2010-mean'] + X[set1[j]]    # Evaluating the sum separately...\n",
    "            X['wind-hgt-850-2010-mean'] = X['wind-hgt-850-2010-mean'] + X[set2[j]]\n",
    "            X['wind-hgt-10-2010-mean'] = X['wind-hgt-10-2010-mean'] + X[set3[j]]\n",
    "            X['wind-hgt-100-2010-mean'] = X['wind-hgt-100-2010-mean'] + X[set4[j]]\n",
    "            X['wind-hgt-500-2010-mean'] = X['wind-hgt-500-2010-mean'] + X[set5[j]]\n",
    "            X['sst-2010-mean'] = X['sst-2010-mean'] + X[set6[j]]\n",
    "        X['icec-2010-mean'] = X['icec-2010-mean'] / 10\n",
    "        X['wind-hgt-850-2010-mean'] = X['wind-hgt-850-2010-mean'] / 10\n",
    "        X['wind-hgt-10-2010-mean'] = X['wind-hgt-10-2010-mean'] / 10     # Calculating the mean...\n",
    "        X['wind-hgt-100-2010-mean'] = X['wind-hgt-100-2010-mean'] / 10\n",
    "        X['wind-hgt-500-2010-mean'] = X['wind-hgt-500-2010-mean'] / 10\n",
    "        X['sst-2010-mean'] = X['sst-2010-mean'] / 10\n",
    "        for j in range(0, len(set1)):    # Removing the unnecessary columns...\n",
    "            X = X.drop(columns=\"icec-2010-{a}\".format(a=j+1))\n",
    "            X = X.drop(columns=\"wind-hgt-850-2010-{a}\".format(a=j+1))\n",
    "            X = X.drop(columns=\"wind-hgt-10-2010-{a}\".format(a=j+1))\n",
    "            X = X.drop(columns=\"wind-hgt-100-2010-{a}\".format(a=j+1))\n",
    "            X = X.drop(columns=\"wind-hgt-500-2010-{a}\".format(a=j+1))\n",
    "            X = X.drop(columns=\"sst-2010-{a}\".format(a=j+1))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhKmixIjp4Q4"
   },
   "source": [
    "#### Now we use the Pipe Classes and thus, call the Pipelining concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKAiD_Tkp4Q5"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline          # Pipeline for preprocessing called...\n",
    "Pipe = Pipeline([                            # Using the Pipeline classes...\n",
    "    (\"Date\", DateToMonthAndYears()),\n",
    "    (\"Index\", RemoveIndex()),\n",
    "    (\"NullValues\", ObtainingNullValues()),     # Every class in Pipeline is called in a Sequential manner...\n",
    "    (\"TwentySet\", TwentyColumnSet()),\n",
    "    (\"TenSet\", TenColumnSet())\n",
    "])\n",
    "dataset = Pipe.fit_transform(dataset)     # Passing the Original dataset into the Pipeline...\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwwmqK7Hp4Q7",
    "outputId": "42a268e3-d136-4ab1-f797-af286c7401c1"
   },
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar0AnpLQp4Q8"
   },
   "source": [
    "#### The number of Columns have been reduced from 246 to 117."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxZOIf6gp4Q9"
   },
   "source": [
    "#### Now we first find the Variation of Temperature in the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SMexwoMp4Q9",
    "outputId": "00566232-8126-4500-a2b1-062bac849e1a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(dataset['contest-tmp2m-14d__tmp2m'], bins=30)\n",
    "plt.xlabel(\"Temperature Bins\", c=\"Red\", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OARRTN6lp4Q-"
   },
   "source": [
    "#### Now we will find the relation between various environmental values and the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PJv2A7gp4Q_"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()    # creating a dataset of environmental factors...\n",
    "data['Evaporation'] = dataset['contest-pevpr-sfc-gauss-14d__pevpr']\n",
    "data['Wind'] = dataset['contest-wind-h10-14d__wind-hgt-10']\n",
    "data['Humidity'] = dataset['contest-rhum-sig995-14d__rhum']\n",
    "data['Temperature'] = dataset['contest-tmp2m-14d__tmp2m']\n",
    "data['SeaLevelPressure'] = dataset['contest-slp-14d__slp']\n",
    "data['Pressure'] = dataset['contest-pres-sfc-gauss-14d__pres']\n",
    "data['Elevation'] = dataset['elevation__elevation']\n",
    "data['Precipitation'] = dataset['contest-precip-14d__precip']\n",
    "data['Month'] = dataset['Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNG17-Psp4Q_",
    "outputId": "614c54ea-b5d2-4519-92ba-b56f1ccfefa0"
   },
   "outputs": [],
   "source": [
    "def PlottingChart():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data.corr(), cmap=\"Spectral\", annot=True)\n",
    "    plt.title(\"Correlation Heatmap of Various Factors\", size=18, c=\"red\")\n",
    "    plt.show()\n",
    "\n",
    "PlottingChart()    # Visualizing data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PlLOEzJp4RA"
   },
   "source": [
    "#### Finding the number of counts of each respective Climatic regions. Since one climatic region quantity is much more, we have to train our model effectively for under-fitting and over-fitting reasons. As for cross validation we can use LeavePOut Cross Validation or Rolling Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38IklQ9np4RA",
    "outputId": "5da4da0e-eb96-43cf-9e11-4f6487b4d974"
   },
   "outputs": [],
   "source": [
    "dataset['climateregions__climateregion'].value_counts().sort_values().plot(kind='bar', figsize=(10,4), rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSPKfXC_p4RE"
   },
   "outputs": [],
   "source": [
    "data['Year'] = dataset['Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-yLWy6ip4RF"
   },
   "source": [
    "#### Since we have many Weather forecast stations in the dataset, so we can first check like which forecast station is having similarity with the Temperature. The best thing to do is by checking their means and standard deviations. If the standard deviation or mean is too high, then we should not take that weather forecast station for training or checking the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbuYCa2qp4RG"
   },
   "outputs": [],
   "source": [
    "temp_mean = data['Temperature'].mean()\n",
    "X = [temp_mean] * 20\n",
    "Y = []\n",
    "stations = ['nmme0-tmp2m-34w__cancm30', 'nmme0-tmp2m-34w__cancm40', 'nmme0-tmp2m-34w__ccsm30', 'nmme0-tmp2m-34w__ccsm40', 'nmme0-tmp2m-34w__cfsv20', 'nmme0-tmp2m-34w__gfdlflora0', 'nmme0-tmp2m-34w__gfdlflorb0', 'nmme0-tmp2m-34w__gfdl0', 'nmme0-tmp2m-34w__nasa0', 'nmme0-tmp2m-34w__nmme0mean']     # Checking for 3-4 weeks...\n",
    "for station in stations:\n",
    "    Y.append(dataset[station].mean())\n",
    "    Y.append(dataset[station].std())\n",
    "Z = []\n",
    "for values in range(0, int(len(X)/2)):\n",
    "    Z.append(\"Mean\")\n",
    "    Z.append(\"Standard Deviation\")\n",
    "NMME = ['Cancm30', 'Cancm30', 'Cancm40', 'Cancm40', 'Ccsm30', 'Ccsm30', 'Ccsm40', 'Ccsm40', 'cfsv20', 'cfsv20','gfd_a', 'gfd_a', 'gfd_b', 'gfd_b', 'gfdl', 'gfdl', 'nasa', 'nasa', 'mean', 'mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtzodZnRp4RH"
   },
   "source": [
    "#### The Pressure vs Temperature contour map is much widespread, and the data mostly is contained within the range of 600 to 900 contour lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6LbI46zp4RH",
    "outputId": "4fef6087-5f00-45e6-c23c-68b1deb76bc9"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = data\n",
    "fig = px.density_contour(df, x=\"Temperature\", y=\"Pressure\", title=\"Pressure X Temperature\")\n",
    "fig.update_traces(contours_coloring=\"fill\", contours_showlabels = True, colorscale=\"Thermal\")\n",
    "fig.update_layout(width=1000, height=750, font_family=\"Courier New\", font_color=\"green\",     title_font_family=\"Times New Roman\", title_font_color=\"red\", title_font_size=24, font_size=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUKe2_b1p4RJ"
   },
   "source": [
    "#### The Elevation vs Temperature contour graph is less widespread and is distinctly classified into two groups. Most of the values are within the range of 500 to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsB68Fuxp4RJ",
    "outputId": "713993ad-6db4-4619-c29d-bcf60d4e5b79"
   },
   "outputs": [],
   "source": [
    "df = data\n",
    "fig = px.density_contour(df, x=\"Temperature\", y=\"Elevation\", title=\"Elevation X Temperature\")\n",
    "fig.update_traces(contours_coloring=\"fill\", contours_showlabels = True, colorscale=\"Thermal\")\n",
    "fig.update_layout(width=1000, height=750, font_family=\"Courier New\", font_color=\"green\",     title_font_family=\"Times New Roman\", title_font_color=\"red\", title_font_size=24, font_size=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwahLhOtp4RK"
   },
   "outputs": [],
   "source": [
    "stations1 = ['nmme0-tmp2m-56w__cancm3', 'nmme0-tmp2m-56w__cancm4', 'nmme0-tmp2m-56w__ccsm3', 'nmme0-tmp2m-56w__ccsm4', 'nmme0-tmp2m-56w__cfsv2', 'nmme0-tmp2m-56w__gfdlflora', 'nmme0-tmp2m-56w__gfdlflorb', 'nmme0-tmp2m-56w__gfdl', 'nmme0-tmp2m-56w__nasa', 'nmme0-tmp2m-56w__nmmemean']      # Checking for 5-6 weeks...\n",
    "Y1 = []\n",
    "for station in stations:\n",
    "    Y1.append(dataset[station].mean())\n",
    "    Y1.append(dataset[station].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UuO3IOtp4RL"
   },
   "outputs": [],
   "source": [
    "stations34 = ['nmme-tmp2m-34w__cancm3', 'nmme-tmp2m-34w__cancm4', 'nmme-tmp2m-34w__ccsm3', 'nmme-tmp2m-34w__ccsm4', 'nmme-tmp2m-34w__cfsv2', 'nmme-tmp2m-34w__gfdl', 'nmme-tmp2m-34w__gfdlflora', 'nmme-tmp2m-34w__gfdlflorb', 'nmme-tmp2m-34w__nasa', 'nmme-tmp2m-34w__nmmemean'] # Checking for 3-4 weeks...\n",
    "Y34 = []\n",
    "for station in stations34:\n",
    "    Y34.append(dataset[station].mean())\n",
    "    Y34.append(dataset[station].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXkwKw3_p4RL"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(X, columns=[\"Temperature\"])\n",
    "df1['StationData'] = Y      # 3-4 week period data for NMME0...\n",
    "df1['Station1Data'] = Y1    # 5-6 week period data...\n",
    "df1['Station2Data'] = Y34   # 3-4 week period data...\n",
    "df1['Stations'] = NMME\n",
    "df1['Values'] = Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8vBQnKZp4RM"
   },
   "source": [
    "#### Since the mean and standard deviation attributes of temperatures is almost constant in the each Month, thus we can clearly divide the data into each Month and then perform the EDA specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWbg-OCTp4RM",
    "outputId": "c8211b53-7d86-4726-bcff-d36496c4b667"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df1, x='Stations', y='StationData', color=\"Values\", title=\"Values for Various Forecast Stations for 3 to 4 Weeks Period for Most Recent Monthly Forecast\", text_auto=True)\n",
    "fig.update_layout(title_font_color=\"purple\", title_font_size=24, font_color=\"green\", font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53htKQx2p4RN"
   },
   "source": [
    "#### Since 5 to 6 week period data is exactly similar, so we will later check whether they are highly correlated. Because sometimes, even if two distributions have same mean and variance, they may not be exactly correlated because in time series data we have a very influential factor as Date and Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JzlK_1Dp4RN",
    "outputId": "b300c522-1604-4170-9bea-fff48db4712a"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df1, x='Stations', y='Station1Data', color=\"Values\", title=\"Values for Various Forecast Stations for 5 to 6 Weeks Period\", text_auto=True)\n",
    "fig.update_layout(title_font_color=\"purple\", title_font_size=24, font_color=\"green\", font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5eET-BZp4RO",
    "outputId": "31ed168f-6165-43e4-dc8b-206dc306e5b6"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df1, x='Stations', y='Station2Data', color=\"Values\", title=\"Values for Various Forecast Stations for 3 to 4 Weeks Period\", text_auto=True)\n",
    "fig.update_layout(title_font_color=\"purple\", title_font_size=24, font_color=\"green\", font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypdjecGFp4RP"
   },
   "source": [
    "#### Intuition from the Bar Plots:-\n",
    "1. Since the means and variance of all the weather forecast stations are almost same, we can exclude many of them and take only one as a parameter while training. Also, when we are training a simple infusion of bias with value of 1 will solve these little discrepancies in the mean while training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQXLIYwFp4RP",
    "outputId": "dd262cbc-d75c-40a4-a08b-48f0b7796490"
   },
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame()\n",
    "data1['Temperature'] = data['Temperature']\n",
    "data1['MeanPrecipitation34'] = dataset['nmme-prate-34w__nmmemean']\n",
    "data1['MeanPrecipitation56'] = dataset['nmme-prate-56w__nmmemean']\n",
    "data1['MeanPrecipitation0of34'] = dataset['nmme0-prate-34w__nmme0mean']\n",
    "data1['MeanPrecipitation0of56'] = dataset['nmme0-prate-56w__nmme0mean']\n",
    "data1['MeanTemperature34'] = dataset['nmme-tmp2m-34w__nmmemean']\n",
    "data1['MeanTemperature56'] = dataset['nmme-tmp2m-56w__nmmemean']\n",
    "data1['MeanTemperature0'] = dataset['nmme0-tmp2m-34w__nmme0mean']\n",
    "data1['MostRecent'] = dataset['nmme0mean']\n",
    "data1['MJOphase'] = dataset['mjo1d__phase']\n",
    "data1['MJOamplitude'] = dataset['mjo1d__amplitude']\n",
    "data1['Region'] = dataset['climateregions__climateregion']\n",
    "data1['MEIrank'] = dataset['mei__meirank']\n",
    "\n",
    "def PlottingChart1():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data1.corr(), cmap=\"magma\", annot=True)\n",
    "    plt.title(\"Correlation Heatmap of Computational Factors\", size=18, c=\"red\")\n",
    "    plt.show()\n",
    "\n",
    "PlottingChart1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpZu7BIrp4RQ"
   },
   "source": [
    "#### Intuition from the Heat Map:- \n",
    "1. Since the Correlation coefficient of Temperature mean of 3-4 weeks and Temperature mean of 5-6 weeks is exactly same (0.95), so we can drop out any one of the group of columns. Here I will be dropping out the 5-6 week columns for all weather forecast stations.\n",
    "2. Also the correlation between MeanTemperature0 and MeanTemperature56 is 0.91 and MeanTemperature0 and MeanTemperature34 is 0.9, so we will remove the 5-6 week entire group of columns.\n",
    "Also I will be dropping out the MeanTemperature0 column and only have its mean value column as a parameter.\n",
    "3. Similarly we will do it for the Precipitation as well since the three columns of Precipitation are highly correlated as well (0.99, 0.84 as correlation coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jYmx4FLp4RQ"
   },
   "outputs": [],
   "source": [
    "class DroppingWeatherStationsI(BaseEstimator, TransformerMixin):    # Removing Weather Stations...\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):      # Removing the 5-6 week Temperature columns...\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__cancm3', axis=1)    # The name of each weather station\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__cancm4', axis=1)    # removed is separately provided...\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__ccsm3', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__ccsm4', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__cfsv2', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__gfdlflora', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__gfdlflorb', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__gfdl', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__nasa', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-56w__nmmemean', axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-l0frD1p4RR"
   },
   "outputs": [],
   "source": [
    "class DroppingWeatherStationsII(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):      # Removing the 3-4 week temperature as well and keeping only the mean column in the dataset...\n",
    "        X = X.drop(columns='nmme-tmp2m-34w__cancm3', axis=1)     \n",
    "        X = X.drop(columns='nmme-tmp2m-34w__cancm4', axis=1)     # The name of each Weather Station removed\n",
    "        X = X.drop(columns='nmme-tmp2m-34w__ccsm3', axis=1)      # is separately provided...\n",
    "        X = X.drop(columns='nmme-tmp2m-34w__ccsm4', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-34w__cfsv2', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-34w__gfdlflora', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-34w__gfdlflorb', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-34w__gfdl', axis=1)\n",
    "        X = X.drop(columns='nmme-tmp2m-34w__nasa', axis=1)    # The Mean column is not removed...\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coaK4LEzp4RR"
   },
   "outputs": [],
   "source": [
    "class DroppingWeatherStationsIII(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     # Removing the NMME0 Temperature 3-4 week columns...\n",
    "        X = X.drop(columns=\"nmme0-tmp2m-34w__cancm30\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-tmp2m-34w__cancm40\", axis=1)\n",
    "        X = X.drop(columns='nmme0-tmp2m-34w__ccsm30', axis=1)    # Name of each removed Weather Station is \n",
    "        X = X.drop(columns='nmme0-tmp2m-34w__ccsm40', axis=1)    # separately provided...\n",
    "        X = X.drop(columns='nmme0-tmp2m-34w__cfsv20', axis=1)\n",
    "        X = X.drop(columns='nmme0-tmp2m-34w__gfdlflora0', axis=1)\n",
    "        X = X.drop(columns='nmme0-tmp2m-34w__gfdlflorb0', axis=1)\n",
    "        X = X.drop(columns='nmme0-tmp2m-34w__gfdl0', axis=1)\n",
    "        X = X.drop(columns='nmme0-tmp2m-34w__nasa0', axis=1)     # Keeping the Mean column as parameter...\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIxEAYv-p4RS"
   },
   "outputs": [],
   "source": [
    "class DroppingWeatherStationsIV(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     # Removing the most recent Temperature forecast columns...\n",
    "        X = X.drop(columns=\"cancm30\", axis=1)\n",
    "        X = X.drop(columns=\"cancm40\", axis=1)\n",
    "        X = X.drop(columns='ccsm30', axis=1)    # Name of each removed Weather Station is \n",
    "        X = X.drop(columns='ccsm40', axis=1)    # separately provided...\n",
    "        X = X.drop(columns='cfsv20', axis=1)\n",
    "        X = X.drop(columns='gfdlflora0', axis=1)\n",
    "        X = X.drop(columns='gfdlflorb0', axis=1)\n",
    "        X = X.drop(columns='gfdl0', axis=1)\n",
    "        X = X.drop(columns='nasa0', axis=1)     # Keeping the Mean column as parameter...\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKOjrrv8p4RT"
   },
   "source": [
    "#### We used a Pipeline to drop the weather stations which we do not need as for training and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55VpipV4p4RU",
    "outputId": "08302d03-635f-4e51-b3d1-255a94ae1da0"
   },
   "outputs": [],
   "source": [
    "Pipe1 = Pipeline([       # Pipeline for removing the Weather Stations created...\n",
    "    (\"DropStationsI\", DroppingWeatherStationsI()),\n",
    "    (\"DropStationsII\", DroppingWeatherStationsII()),\n",
    "    (\"DropStationsIII\", DroppingWeatherStationsIII()),     # Integrating the required classes...\n",
    "    (\"DropStationsIV\", DroppingWeatherStationsIV())\n",
    "])\n",
    "dataset = Pipe1.fit_transform(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSHCfNnip4RU"
   },
   "source": [
    "#### The dataset is now reduced to from 117 to 89 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9ohTnfdp4RV",
    "outputId": "5f026481-5d62-4a95-d900-a15a483e91df"
   },
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame()\n",
    "data2['Temperature'] = data1['Temperature']\n",
    "data2['ZonalWind250'] = dataset['contest-wind-uwnd-250-14d__wind-uwnd-250']\n",
    "data2['ZonalWind925'] = dataset['contest-wind-uwnd-925-14d__wind-uwnd-925']\n",
    "data2['LongitudeWind250'] = dataset['contest-wind-vwnd-250-14d__wind-vwnd-250']\n",
    "data2['LongitudeWind925'] = dataset['contest-wind-vwnd-925-14d__wind-vwnd-925']\n",
    "data2['Height10'] = dataset['contest-wind-h10-14d__wind-hgt-10']\n",
    "data2['Height100'] = dataset['contest-wind-h100-14d__wind-hgt-100']\n",
    "data2['Height500'] = dataset['contest-wind-h500-14d__wind-hgt-500']\n",
    "data2['Height850'] = dataset['contest-wind-h850-14d__wind-hgt-850']\n",
    "data2['Evaporation'] = data['Evaporation']\n",
    "\n",
    "def PlottingChart2():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data2.corr(), cmap=\"icefire\", annot=True)\n",
    "    plt.title(\"Correlation Heatmap of Computational Factors\", size=18, c=\"red\")\n",
    "    plt.show()\n",
    "\n",
    "PlottingChart2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN8k0UVLp4RW"
   },
   "source": [
    "#### The Contour Lines are dispersed into a dense set of two regions, thus the data mostly lies between te height of (16.2k to 16.4k) Ist set and (16.6k to 16.7k) IInd set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZVlAdKlp4RW",
    "outputId": "efd86395-9dc7-407c-921e-06e31ecbbe6c"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = data2\n",
    "fig = px.density_contour(df, x=\"Height100\", y=\"ZonalWind250\", title=\"Height100 Millibars X Zonal Wind 250\")\n",
    "fig.update_traces(contours_coloring=\"fill\", contours_showlabels = True, colorscale=\"ylgnbu_r\")\n",
    "fig.update_layout(width=900, height=550, font_family=\"Courier New\", font_color=\"green\",     title_font_family=\"Times New Roman\", title_font_color=\"red\", title_font_size=24, font_size=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef3gewTNp4RX"
   },
   "source": [
    "#### The Contour lines are quite linear in shape and the distribution is pretty constant and enclosed within a single large group with Zonal Wind range (-1 to 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBVZkxMEp4RX",
    "outputId": "825982cc-eee1-4fe8-8e2a-1516c2d7513d"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = data2\n",
    "fig = px.density_contour(df, x=\"Height500\", y=\"ZonalWind925\", title=\"Height 500 Millibars X ZonalWind925\")\n",
    "fig.update_traces(contours_coloring=\"fill\", contours_showlabels = True, colorscale=\"ylgnbu_r\")\n",
    "fig.update_layout(width=900, height=550, font_family=\"Courier New\", font_color=\"green\",     title_font_family=\"Times New Roman\", title_font_color=\"red\", title_font_size=24, font_size=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiAyVfEsp4RY",
    "outputId": "25c3b5bd-192d-4977-e025-4c0c12d26979"
   },
   "outputs": [],
   "source": [
    "ElNino = pd.DataFrame()\n",
    "# Creating the Mean of El Nino dataset of the 2010-2020 when it prevailed...\n",
    "ElNino['Temperature'] = data1['Temperature']\n",
    "ElNino['ElNino-Mean-LongitudeWind250'] = dataset['wind-vwnd-250-2010-mean']\n",
    "ElNino['ElNino-Mean-LongitudeWind925'] = dataset['wind-vwnd-925-2010-mean']\n",
    "ElNino['ElNino-Mean-ZonalWind250'] = dataset['wind-uwnd-250-2010-mean']\n",
    "ElNino['ElNino-Mean-ZonalWind925'] = dataset['wind-uwnd-925-2010-mean']\n",
    "ElNino['Mean-GlacierFactor'] = dataset['icec-2010-mean']\n",
    "ElNino['ElNino-Mean-Height10'] = dataset['wind-hgt-10-2010-mean']\n",
    "ElNino['ElNino-Mean-Height100'] = dataset['wind-hgt-100-2010-mean']\n",
    "ElNino['ElNino-Mean-Height500'] = dataset['wind-hgt-500-2010-mean']\n",
    "ElNino['ElNino-Mean-Height850'] = dataset['wind-hgt-850-2010-mean']\n",
    "ElNino['Mean-SeaTemperature'] = dataset['sst-2010-mean']\n",
    "\n",
    "def PlottingChartElNino():          # Plotting the Heatmap...\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(ElNino.corr(), cmap=\"icefire\", annot=True)\n",
    "    plt.title(\"Correlation Heatmap of ElNino (2010-2020) to Environment Factors\", size=22, c=\"red\")\n",
    "    plt.show()\n",
    "\n",
    "ElNino['Month'] = dataset['Month']\n",
    "PlottingChartElNino()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbcLHmdAp4RY"
   },
   "source": [
    "#### Now we will reduce the number of Precipitation rate columns as well as we did for the Temperature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWS25Qqkp4RZ"
   },
   "outputs": [],
   "source": [
    "Y = []\n",
    "precip = ['nmme-prate-34w__cancm3', 'nmme-prate-34w__cancm4','nmme-prate-34w__ccsm3','nmme-prate-34w__ccsm4','nmme-prate-34w__cfsv2', 'nmme-prate-34w__gfdl','nmme-prate-34w__gfdlflora','nmme-prate-34w__gfdlflorb','nmme-prate-34w__nasa', 'nmme-prate-34w__nmmemean']     # Checking for 3-4 weeks...\n",
    "for station in precip:\n",
    "    Y.append(dataset[station].mean())\n",
    "    Y.append(dataset[station].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3CT9dCXp4RZ"
   },
   "outputs": [],
   "source": [
    "Y1 = []\n",
    "precip1 = ['nmme-prate-56w__cancm3', 'nmme-prate-56w__cancm4','nmme-prate-56w__ccsm3','nmme-prate-56w__ccsm4','nmme-prate-56w__cfsv2', 'nmme-prate-56w__gfdl','nmme-prate-56w__gfdlflora','nmme-prate-56w__gfdlflorb','nmme-prate-56w__nasa', 'nmme-prate-56w__nmmemean']     # Checking for 5-6 weeks...\n",
    "for station in precip1:\n",
    "    Y1.append(dataset[station].mean())\n",
    "    Y1.append(dataset[station].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsoA-sJwp4Ra"
   },
   "outputs": [],
   "source": [
    "Y2 = []\n",
    "precip2 = ['nmme0-prate-34w__cancm30', 'nmme0-prate-34w__cancm40','nmme0-prate-34w__ccsm30','nmme0-prate-34w__ccsm40','nmme0-prate-34w__cfsv20', 'nmme0-prate-34w__gfdl0','nmme0-prate-34w__gfdlflora0','nmme0-prate-34w__gfdlflorb0','nmme0-prate-34w__nasa0', 'nmme0-prate-34w__nmme0mean']     # Checking for 3-4  weeks of most recent (NMME0)...\n",
    "for station in precip2:\n",
    "    Y2.append(dataset[station].mean())\n",
    "    Y2.append(dataset[station].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO8WYMoCp4Ra"
   },
   "outputs": [],
   "source": [
    "Y3 = []\n",
    "precip3 = ['nmme0-prate-56w__cancm30', 'nmme0-prate-56w__cancm40','nmme0-prate-56w__ccsm30','nmme0-prate-56w__ccsm40','nmme0-prate-56w__cfsv20', 'nmme0-prate-56w__gfdl0','nmme0-prate-56w__gfdlflora0','nmme0-prate-56w__gfdlflorb0','nmme0-prate-56w__nasa0', 'nmme0-prate-56w__nmme0mean']     # Checking for 5-6  weeks of most recent (NMME0)...\n",
    "for station in precip3:\n",
    "    Y3.append(dataset[station].mean())\n",
    "    Y3.append(dataset[station].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27Nedwngp4Rb",
    "outputId": "5deda441-94e0-4193-f74e-b2ec2b6b1388"
   },
   "outputs": [],
   "source": [
    "precipitate = pd.DataFrame(Z, columns=[\"values\"])\n",
    "precipitate[\"Stations\"] = NMME            # Stations list...\n",
    "precipitate[\"Precip1\"] = Y               # Column for 3-4 week mean precipitation...\n",
    "precipitate['Precip2'] = Y1              # Column for 5-6 week mean precipitation...\n",
    "precipitate[\"Precip3\"] = Y2              # Column for 3-4 week most recent mean precipitation NMME0...\n",
    "precipitate[\"Precip4\"] = Y3              # Column for 5-6 week most recent mean precipitation NMME0...\n",
    "precipitate.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLXGj0hCp4Rb",
    "outputId": "b073af74-ff3e-4856-ecc9-5afd3977c4e3"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(precipitate, x='Stations', y='Precip1', color=\"values\", title=\"Values for Various Forecast Stations for 3 to 4 Weeks Precipitation Rate\", text_auto=True)\n",
    "fig.update_layout(title_font_color=\"purple\", title_font_size=24, font_color=\"green\", font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ybw8TOVp4Rc",
    "outputId": "c7ad6d21-6c51-41fd-8a48-c41aa63841a2"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(precipitate, x='Stations', y='Precip2', color=\"values\", title=\"Values for Various Forecast Stations for 5 to 6 Weeks Precipitation Rate\", text_auto=True)\n",
    "fig.update_layout(title_font_color=\"purple\", title_font_size=24, font_color=\"green\", font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBAqoZMDp4Rc"
   },
   "source": [
    "#### The above two bar plots have exactly same mean and standard deviation and are exactly correlated (0.99). Thus, we can remove any one of the above groups for Precipitation Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKWl2BxDp4Rd",
    "outputId": "29a8ce59-ffba-4154-d7cd-269efb3b3b81"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(precipitate, x='Stations', y='Precip3', color=\"values\", title=\"Values for Various Forecast Stations for 3 to 4 Weeks Most Recent Precipitation Rate\", text_auto=True)\n",
    "fig.update_layout(title_font_color=\"purple\", title_font_size=24, font_color=\"green\", font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqvVqan8p4Rd",
    "outputId": "b733fa63-eb74-4bbe-9cae-9dca490706ea"
   },
   "outputs": [],
   "source": [
    "fig = px.bar(precipitate, x='Stations', y='Precip4', color=\"values\", title=\"Values for Various Forecast Stations for 5 to 6 Weeks Most Recent Precipitation Rate\", text_auto=True)\n",
    "fig.update_layout(title_font_color=\"purple\", title_font_size=24, font_color=\"green\", font_size=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI7CHkC0p4Re"
   },
   "source": [
    "#### The above two bar plots have almost same mean and standard deviation and are highly correlated (0.92). Thus, we can remove any one of the above groups for Precipitation Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQVj7bAgp4Re"
   },
   "outputs": [],
   "source": [
    "class PrecipitationDehydratedI(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     # Removing the entire column set of 3-4 Week Precipitation...\n",
    "        X = X.drop(columns=\"nmme-prate-34w__cancm3\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__cancm4\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__ccsm3\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__ccsm4\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__cfsv2\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__gfdl\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__gfdlflora\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__gfdlflorb\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__nasa\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-34w__nmmemean\", axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqdAfEvlp4Rf"
   },
   "source": [
    "#### I am keeping two columns here as parameters one of the lowest and one of the highest, since the means and deviations are widespread in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqGJjZT8p4Rf"
   },
   "outputs": [],
   "source": [
    "class PrecipitationDehydratedII(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     # Keeping cancm4 and ccsm3 columns for parameters...\n",
    "        X = X.drop(columns=\"nmme-prate-56w__cancm3\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-56w__ccsm4\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-56w__cfsv2\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-56w__gfdl\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-56w__gfdlflora\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-56w__gfdlflorb\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-56w__nasa\", axis=1)\n",
    "        X = X.drop(columns=\"nmme-prate-56w__nmmemean\", axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruTsUx9fp4Rg"
   },
   "outputs": [],
   "source": [
    "class PrecipitationDehydratedIII(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     # Removing the entire column set of 3-4 Week most recent Precipitation...\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__cancm30\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__cancm40\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__ccsm30\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__ccsm40\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__cfsv20\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__gfdl0\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__gfdlflora0\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__gfdlflorb0\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__nasa0\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-34w__nmme0mean\", axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELN18is1p4Rg"
   },
   "source": [
    "#### I am keeping two columns here as parameters one of the lowest and one of the highest, since the means and deviations are widespread in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3NypLTLp4Rg"
   },
   "outputs": [],
   "source": [
    "class PrecipitationDehydratedIV(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):     # Keeping cancm4 and ccsm3 columns for parameters...\n",
    "        X = X.drop(columns=\"nmme0-prate-56w__cancm30\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-56w__ccsm40\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-56w__cfsv20\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-56w__gfdl0\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-56w__gfdlflora0\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-56w__gfdlflorb0\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-56w__nasa0\", axis=1)\n",
    "        X = X.drop(columns=\"nmme0-prate-56w__nmme0mean\", axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2G-piuMp4Rh"
   },
   "source": [
    "#### We use another Pipeline to remove the insignificant or repetitive precipitation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPdJ6zeUp4Rh",
    "outputId": "23660eab-13bc-4289-e276-b5e27aaa669b"
   },
   "outputs": [],
   "source": [
    "Precipitation = Pipeline([     # Using the Pipeline to call for specific classes... \n",
    "    (\"Dehydrate1\", PrecipitationDehydratedI()),\n",
    "    (\"Dehydrate2\", PrecipitationDehydratedII()),\n",
    "    (\"Dehydrate3\", PrecipitationDehydratedIII()),\n",
    "    (\"Dehydrate4\", PrecipitationDehydratedIV()),\n",
    "])\n",
    "dataset = Precipitation.fit_transform(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwnSTe9fp4Ri",
    "outputId": "84c69bae-7b85-4099-d695-e4168eb35751"
   },
   "outputs": [],
   "source": [
    "for i in dataset.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjgJ2Fvap4Ri"
   },
   "outputs": [],
   "source": [
    "'''dataset.to_csv(\"E:/Downloads/updated_train_data.csv\", index=False)'''\n",
    "# Use this cell only when you want to download the processed dataset..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "87970ed34195fef68c8464c6d4e1428c2376dd29de258c9c3dc6f07b1b1cbaab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
